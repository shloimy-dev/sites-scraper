# Toys4U2 — Product Data Scraper

Scrapes product data (title, description, image) from manufacturer websites for each product in our inventory sheets.

## Results

**10 sites scraped successfully** — data is in `data/ready/`:

| Site | Products | Images | Descriptions |
|------|----------|--------|--------------|
| aurora | 413 | 413 | 413 |
| bazic | 382 | 382 | 382 |
| chazak | 319 | 319 | 77 |
| playkidiz | 93 | 92 | 91 |
| bruder | 71 | 71 | 71 |
| atiko | 28 | 0 | 19 |
| samvix | 24 | 24 | 24 |
| metal_earth | 12 | 12 | 12 |
| microkick | 7 | 7 | 7 |
| razor | 4 | 4 | 0 |

**9 sites could not be scraped** (Cloudflare blocking, site down, no product pages, etc.):
colours_craft, enday, gi_go, goplay, lchaim, moore, rhode_island, sands, winning_moves.
See `docs/sites/<site>.md` for each site's analysis.

## Project Structure

```
config/sites.yaml          # Site definitions (id, base_url, sheet name)
data/
  sheets/                  # Input CSVs — product lists with UPC codes and names
  ready/
    extracted/             # Final scraped data (CSV per site)
    images/                # Downloaded product images (folder per site)
scripts/
  deep_analyze.py          # Analyzes all sites: tests 6+ URL strategies per site,
                           #   verifies data is truly per-product (not generic/homepage)
  scraper_lib.py           # Shared utilities (CSV I/O, extraction, image download)
  run_scrapers.py          # Runs all scrapers in parallel batches
  sites/                   # One scraper per site
    scrape_bruder.py
    scrape_chazak.py
    scrape_metal_earth.py
    scrape_microkick.py
    scrape_playkidiz.py
    scrape_razor.py
    scrape_samvix.py
docs/sites/                # Per-site analysis specs (generated by deep_analyze.py)
```

## How It Works

1. **Analyze** — `deep_analyze.py` tests each site with 3 sample products across multiple URL strategies (direct URL, search by UPC, search by name). It compares results against the homepage baseline and checks that different products get different data. Only strategies that produce truly unique per-product results score well.

2. **Scrape** — Each site has a dedicated scraper in `scripts/sites/` that uses the proven strategy for that site. Scrapers search, follow product links, extract from JSON-LD or CSS selectors, and verify data before saving.

3. **Run** — `run_scrapers.py` runs all scrapers in parallel (3 at a time). Output goes to `data/extracted/` and `data/images/`, then gets moved to `data/ready/` when verified.

## Setup

```bash
pip install -r requirements.txt
python3 -m playwright install chromium
```
